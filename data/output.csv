id,type,title-text,content,links,icon
0.3112686162964079,nodeTitle,Transformer architectures,,,
0.4223686162964079,nodeTitle,Tokenizer,,,
0.5334686162964079,nodeTitle,Positional encoders,,,
0.64457686162964079,nodeTitle,Basic attention,,,
0.689886162964079,nodeTitle,Training,,,
0.686162964079,nodeTitle,Inferencing,,,
0.6686162964079,nodeTitle,Size reduction,,,
0.6945686162964079,nodeTitle,Interpretability,,,
0.6845686162964079,nodeTitle,Coding libraries,,,
0.5572621926782715,nodeTitle,LLMs,,,
0.16837110594925808,nodeIcon,Encoder,,,
0.9535163345224653,nodeIcon,MoE,,,
0.9119649186997962,nodeIcon,Decoder,,,
0.36003004998950416,nodeText,Casual,,,
0.6743400871686738,nodeText,Prefix,,,
0.9819024527412874,nodeText,Mixture of depths,,,
0.4285567861537116,nodeText,Sparse,,,
0.2552518656893066,nodeText,JET,,,
0.4204944704112741,nodeText,DeepSeek,,,
0.8694107478587487,nodeText,Multi-head,,,
0.30107243113130644,nodeText,Context window extension,,,
0.01878261236255252,nodeText,RoPE,,,
0.23022156357560264,nodeText,ALiBi,,,
0.8671703240144155,nodeText,Efficient attention mechanism,,,
0.2921099962836504,nodeText,Transient global attention,,,
0.21959857164052599,nodeIcon,Attention acceleration,,,
0.9205533533188854,nodeIcon,Sparse attention,,,
0.06602808994944565,nodeIcon,Group query attention,,,
0.45473272314021984,nodeIcon,Multi-query attention (PaLM),,,
0.45515955241545836,nodeIcon,Self-attention,,,
0.6372481421035265,nodeText,Cross atttention,,,
0.685187248072249,nodeText,Masked attention,,,
0.3447686499935707,nodeText,Multi-head attention,,,
0.6934752898121264,nodeText,Parallel attention,,,
0.22843460540858906,nodeText,Multi-head latent attention,,,
0.5552244213729263,nodeText,Flash attention,,,
0.7332071927522315,nodeIcon,General training objective,,,
0.37138659130864626,nodeIcon,Training stages,,,
0.5900867524030164,nodeIcon,Data and efficiency,,,
0.5032271243153694,nodeText,Full language modeling,,,
0.47633537153863204,nodeText,Prefix language modeling,,,
0.09209548621139207,nodeText,Masked language modeling,,,
0.3738819592763454,nodeText,Unified language modeling,,,
0.4633230592648996,nodeText,Pre-training,,,
0.05627837289071924,nodeText,Fine tuning,,,
0.7163765204902184,nodeText,Continue pre-training,,,
0.6393123989211589,nodeText,Framework,,,
0.38704478905124673,nodeText,PEFT,,,
0.6504105118867951,nodeText,LoRA,,,
0.3013969688629039,nodeText,Representing-editing,,,
0.49372307422653017,nodeText,Model merging,,,
0.0031744518739229566,nodeText,Dynamic rank,,,
0.3804064581992368,nodeText,Alignment,,,
0.9139380508532906,nodeText,Instruction tuning,,,
0.3373668642401584,nodeIcon,Tool augmented LLMs,,,
0.11629321379045243,nodeIcon,LLM agents,,,
0.7316342521599828,nodeIcon,Hallucinations,,,
0.9529174964321816,nodeIcon,RAG,,,
0.7718368233931456,nodeIcon,Prompting,,,
0.47398047029179713,nodeText,Single-turn instructions,,,
0.12928728870139694,nodeText,Reasoning,,,
0.9171671506581673,nodeText,Prompt engineering,,,
0.17057965717857604,nodeText,Multi-turn instruction,,,
0.8243438091162589,nodeText,In-context learning,,,
0.6326985573947084,nodeText,Reasoning,,,
0.036732493389526555,nodeText,Self consistency,,,
0.7271751646030982,nodeText,Chain of thought,,,
0.6400548296907231,nodeText,Tree of thought,,,
0.8097548908447685,nodeIcon,Pruning,,,
0.38025724166233865,nodeIcon,Quantization,,,
0.6305598727274426,nodeText,Position interpolation,,,
0.9738787234747688,nodeText,Sliding window,,,
0.6075089421173911,nodeIcon,Dataset,,,
0.7506624523475782,nodeText,Chinchilla,,,
0.5920869136350291,nodeText,Efficient instruction learning,,,
0.1950495719620955,nodeText,Mode switching,,,
0.6981935635634451,nodeText,Training parallelism,,,
0.9246783246029247,nodeText,Training instability,,,
0.3448182373782269,nodeText,Spare vs. dense activated,,,
0.16464514976496947,nodeText,Layer normalization,,,
0.8465400537657881,nodeText,Overfitting,,,
0.9019673398483306,nodeText,Non-PEFT,,,
0.8022936991735767,nodeText,Continual learning,,,
0.2571558418319977,nodeText,LLama factory,,,
0.7106048977929156,nodeText,ReFT,,,
0.9309081137850372,nodeText,Evoluation optimization (model merging),,,
0.4938689570539174,nodeText,Multiple LoRA,,,
0.36536691276825106,nodeText,Quantization,,,
0.49855180317787373,nodeText,Standard designs,,,
0.8134580383080576,nodeText,Memory efficient,,,
0.3534611595702071,nodeText,Pruning,,,
0.9727134851277237,nodeText,LoRA Pruning,,,
0.4627673329164894,nodeText,LoRA,,,
0.17335594685346933,nodeText,KronA,,,
0.41545166834432323,nodeText,DoRA,,,
0.03693524715835261,nodeText,VeRA,,,
0.03952217297120675,nodeText,DyLoRA,,,
0.6236993713414836,nodeText,AdaLoRA,,,
0.28945961397073594,nodeText,LoRAHub,,,
0.33617940998778506,nodeText,MixLoRA,,,
0.8564815257405871,nodeText,MoLE,,,
0.7997263977142455,nodeText,MoLoRA,,,
0.024459453942894616,nodeText,QLoRA,,,
0.9867286477548458,nodeText,Adapter,,,
0.010244207309925146,nodeText,QA-LoRA,,,
0.17872216402419006,nodeText,LoftQ,,,
0.9817681001692797,nodeText,BitDelta,,,
0.1729271544031492,nodeText,Side tuning,,,
0.2553297951836566,nodeText,LST,,,
0.12022691400256758,nodeText,GaLore,,,
0.22104755575372748,nodeText,Soft prompt,,,
0.5448035338081827,nodeText,Text,,,
0.3039132668590998,nodeText,Design,,,
0.8863073430140549,nodeText,Serial adapter,,,
0.2333537145169522,nodeText,Parallel adapter,,,
0.872269484747318,nodeText,CoDA,,,
0.9015839688098735,nodeText,Adapter layer,,,
0.2917524045831028,nodeText,Multi-task adaptation,,,
0.389812332240705,nodeText,LoRA-adapter hybrid,,,
0.6129471352161286,nodeText,Adapter fusion,,,
0.19759796818855402,nodeText,Hyperformer,,,
0.7218403811936429,nodeText,ATTEMPT,,,
0.25017787379186385,nodeText,MPT,,,
0.6094425326241595,nodeText,UNIPELT,,,
0.7575967315621617,nodeText,NOAH,,,
0.3426444578255132,nodeText,LLM adapters,,,
0.9585248244664728,nodeText,Design,,,
0.7683008680124852,nodeText,P tuning,,,
0.6735095492475287,nodeText,P tuning V2,,,
0.37715614227858585,nodeText,Prefix tuning,,,
0.31318051963659177,nodeText,Prompt tuning,,,
0.25248419480039375,nodeText,Training speedups,,,
0.879723952713322,nodeText,Spot,,,
0.5419441057218393,nodeText,TPT,,,
0.0028277572579011867,nodeText,Cleaning methods,,,
0.9099953194265404,nodeText,Popular data sets,,,
0.40096721633648524,nodeText,Benchmark,,,
0.008089967442346913,nodeText,General pretraining,,,
0.3890923368934718,nodeText,Instruction tuning,,,
0.02109063700935243,nodeText,Types,,,
0.2472569569290033,nodeText,Multitask,,,
0.5467377306867194,nodeText,Language understanding,,,
0.11653021955174792,nodeText,Story cloze,,,
0.44089624702914376,nodeText,World understanding,,,
0.23539831457274274,nodeText,Contextual understanding,,,
0.8427333300733013,nodeText,Commonsense,,,
0.4549495844719449,nodeText,Reading comprehension,,,
0.2871421002413348,nodeText,Reading comprehension,,,
0.2859584404292621,nodeText,Mathematical reasoning,,,
0.3048298842172015,nodeText,Cross-lingual understanding,,,
0.9906265528008249,nodeText,Truthfulness,,,
0.31911932031189383,nodeText,Bias & ethics,,,
0.45577653547610786,nodeText,Few shot,,,
0.9443263538994278,nodeText,Zero shot,,,
0.3634236678222007,nodeText,MMLU,,,
0.20024225951861752,nodeText,SuperGLUE,,,
0.7120153861538088,nodeText,BIG bench,,,
0.7517912605384311,nodeText,GLUE,,,
0.6802899267298923,nodeText,WinoGrande,,,
0.4337071301832487,nodeText,CoQA,,,
0.17965971532675273,nodeText,WiC,,,
0.4551764980407209,nodeText,Wikitext103,,,
0.19975190346761273,nodeText,PG19,,,
0.7217477557148058,nodeText,C4,,,
0.9038031607513244,nodeText,LCQMC,,,
0.9651689711284379,nodeText,LAMBADA,,,
0.20130928519180968,nodeText,TriviaQA,,,
0.2790262840437916,nodeText,PIQA,,,
0.942474427461079,nodeText,ARC,,,
0.1940737509512962,nodeText,RACE middle,,,
0.4047405202533825,nodeText,RACE,,,
0.8847362642663543,nodeText,RACE high,,,
0.17889805519190682,nodeText,QuAC,,,
0.1659700835408282,nodeText,GSM8K,,,
0.5492359296818146,nodeText,Math23k,,,
0.9856555884253293,nodeText,MATH,,,
0.6189272007297173,nodeText,SQUADv2,,,
0.7974420832411726,nodeText,DROP,,,
0.678950309565491,nodeText,BooIQ,,,
0.9065273922178909,nodeText,RTE,,,
0.03811301554789015,nodeText,WebQA,,,
0.7768554377904022,nodeText,CMRC2018,,,
0.5382428979435236,nodeText,HellaSwag,,,
0.7350455832627647,nodeText,COPA,,,
0.03362407117455435,nodeText,WSC,,,
0.9831129155868468,nodeText,CSQA,,,
